{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d35499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import gymnasium_robotics\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import random\n",
    "import os\n",
    "from utils.functions import ohe_goals, flatten_info, add_human_demonstrations\n",
    "from utils.checkpoint import save_checkpoint, load_checkpoint\n",
    "from utils.agent import DDPGAgent\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "# DETERMINISM SETUP\n",
    "SEED = 42\n",
    "task_rng = np.random.default_rng(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "torch.use_deterministic_algorithms(True, warn_only=True)\n",
    "\n",
    "\n",
    "# ENV AND TRAINING PARAMETERS\n",
    "ENV_NAME = \"FrankaKitchen-v1\"\n",
    "MAX_EPISODES = 20000\n",
    "MAX_STEPS = 1000\n",
    "BATCH_SIZE = 512\n",
    "START_TRAINING_AFTER = 3000\n",
    "NOISE = 0.05\n",
    "K = 1\n",
    "HUMAN_DATA_FREQ = 500\n",
    "CHECKPOINT_FREQ = 500\n",
    "PLOT_FREQ = 1\n",
    "RESUME_PATH = \"checkpoints/checkpoint_1500.pth\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "PARAMETER_NOISE = False\n",
    "TASKS = [\n",
    "    \"microwave\",\n",
    "    \"kettle\",\n",
    "    \"light switch\",\n",
    "    \"slide cabinet\",\n",
    "    \"hinge cabinet\",\n",
    "    \"top burner\",\n",
    "    \"bottom burner\",\n",
    "]\n",
    "\n",
    "# creating ohe lookup table, which is crucial for the agent!\n",
    "# agent should understand what is the aimed task of the (s,a,r) tuple\n",
    "# therefore, we give current task info with OHE vector as a feature\n",
    "# so that the agent can distinguish which actions should be taken for a specific task.\n",
    "OHE_LOOKUP = ohe_goals(TASKS)\n",
    "# what about relabled \"fake\" tasks in HER mechanism?\n",
    "# we simply use a zero vector for those.\n",
    "\n",
    "# CREATING ENVIRONMENT AND INITIALIZING AGENT\n",
    "env = gym.make(ENV_NAME, max_episode_steps=MAX_STEPS, tasks_to_complete=TASKS)\n",
    "env.action_space.seed(SEED)\n",
    "obs_dict = env.reset()[0]\n",
    "state_dim = obs_dict[\"observation\"].shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "sample_goal = np.concatenate([obs_dict[\"desired_goal\"][k] for k in TASKS])\n",
    "goal_dim = sample_goal.shape[0]\n",
    "ohe_dim = len(TASKS)\n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "agent = DDPGAgent(state_dim, action_dim, ohe_dim, goal_dim, device)\n",
    "\n",
    "task_history = {task: [0] for task in TASKS}\n",
    "cumulative_victories = {task: 0 for task in TASKS}\n",
    "loss_history = {\"critic_loss\": [], \"actor_loss\": []}\n",
    "q_value_history = []\n",
    "efficiency_history = {task: [] for task in TASKS}\n",
    "\n",
    "start_episode = 1\n",
    "total_steps = 0\n",
    "\n",
    "plot_display = display.display(plt.figure(figsize=(16, 12)), display_id=True)\n",
    "\n",
    "# If RESUME_PATH exist, checkpoint is loaded \n",
    "(\n",
    "    start_episode,\n",
    "    task_history,\n",
    "    cumulative_victories,\n",
    "    loss_history,\n",
    "    q_value_history,\n",
    "    efficiency_history,\n",
    "    total_steps,\n",
    "    replay_buffer,\n",
    ") = load_checkpoint(CHECKPOINT_DIR, RESUME_PATH, agent)\n",
    "\n",
    "# IMPORTANT\n",
    "# comment the replay buffer line below if you want to use saved replay buffer!\n",
    "replay_buffer = ReplayBuffer(\n",
    "    state_dim, action_dim, ohe_dim, goal_dim, device, max_size=2_000_000, seed=SEED\n",
    ")\n",
    "\n",
    "# for HER, we will use this zero vector instead of OHE\n",
    "# because HER is not task specific.\n",
    "generic_task_ohe = np.zeros(ohe_dim)\n",
    "\n",
    "print(f\"State Dim: {state_dim}, Goal Dim: {goal_dim}, OHE Dim: {ohe_dim}\")\n",
    "print(f\"Running On Device: {device}\")\n",
    "\n",
    "add_human_demonstrations(replay_buffer, OHE_LOOKUP, TASKS)\n",
    "\n",
    "for episode in range(start_episode, MAX_EPISODES):\n",
    "    raw_state, _ = env.reset(seed=SEED)\n",
    "    state, achieved_goal, desired_goal = flatten_info(raw_state, TASKS)\n",
    "\n",
    "    episode_cache = []\n",
    "    victory_map = {}\n",
    "    current_start_index = 0\n",
    "    undone_tasks = list(TASKS)\n",
    "\n",
    "    ep_critic_losses = []\n",
    "    ep_actor_losses = []\n",
    "    ep_q_values = []\n",
    "\n",
    "    current_task_name = task_rng.choice(undone_tasks)\n",
    "    current_ohe = OHE_LOOKUP[current_task_name]\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        action = agent.select_action(\n",
    "            state,\n",
    "            current_ohe,\n",
    "            desired_goal,\n",
    "            noise=NOISE,\n",
    "            use_parameter_noise=PARAMETER_NOISE,\n",
    "        )\n",
    "\n",
    "        next_raw_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state, next_achieved, next_desired = flatten_info(next_raw_state, TASKS)\n",
    "\n",
    "        # we add all steps into episode cache\n",
    "        # episode cache is also used for HER later.\n",
    "        episode_cache.append(\n",
    "            {\n",
    "                \"state\": state,\n",
    "                \"action\": action,\n",
    "                \"next_state\": next_state,\n",
    "                \"achieved_goal\": next_achieved,\n",
    "                \"desired_goal\": next_desired,\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # checking if \"ANY\" task is completed at current step.\n",
    "        # because agent can complete other tasks too.\n",
    "        if info[\"step_task_completions\"]:\n",
    "            for task_name in info[\"step_task_completions\"]:\n",
    "                # it's unlikely, impossible, to complete more than 1 task in just 1 step\n",
    "                # but we are using for loop just in case.\n",
    "                if task_name not in victory_map:\n",
    "                    victory_map[task_name] = step\n",
    "\n",
    "                    duration = step - current_start_index\n",
    "                    efficiency_history[task_name].append(duration)\n",
    "\n",
    "                    # agent can solve a different task!\n",
    "                    # hence, we will pretend the agent was aimed for that goal\n",
    "                    # starting from step = current start index\n",
    "                    # we obtain the solved task's OHE vector for relabeling\n",
    "                    solved_ohe = OHE_LOOKUP[task_name]\n",
    "\n",
    "                    for i in range(current_start_index, step + 1):\n",
    "                        data = episode_cache[i]\n",
    "                        \n",
    "                        # reward is given at the end.\n",
    "                        # other states' reward are zero \n",
    "                        is_finish = i == step\n",
    "                        \n",
    "                        replay_buffer.add(\n",
    "                            data[\"state\"],\n",
    "                            data[\"action\"],\n",
    "                            solved_ohe,\n",
    "                            data[\"desired_goal\"],\n",
    "                            data[\"next_state\"],\n",
    "                            reward=1 if is_finish else 0,\n",
    "                            done=1 if is_finish else 0,\n",
    "                        )\n",
    "                        # we keep the done info for the q_value update!\n",
    "                        # please check utils/agent.py\n",
    "\n",
    "                    current_start_index = step + 1\n",
    "                    # if agent will complete another task after this step\n",
    "                    # we should treat current step as the beggining for that task for adding into buffer\n",
    "                    # hence, we update the current start index variable with step+1.\n",
    "                    if task_name in undone_tasks:\n",
    "                        undone_tasks.remove(task_name)\n",
    "\n",
    "                    if undone_tasks:\n",
    "                        current_task_name = task_rng.choice(undone_tasks)\n",
    "                        current_ohe = OHE_LOOKUP[current_task_name]\n",
    "\n",
    "            if len(info[\"episode_task_completions\"]) == len(TASKS):\n",
    "                # if all tasks are completed...\n",
    "                break\n",
    "\n",
    "        state = next_state\n",
    "        total_steps += 1\n",
    "        \n",
    "        # training starts after observing \"START_TRAINING_AFTER\" steps.\n",
    "        if total_steps > START_TRAINING_AFTER:\n",
    "            for _ in range(K):\n",
    "                c_loss, a_loss, q_val = agent.train(replay_buffer, BATCH_SIZE)\n",
    "                ep_critic_losses.append(c_loss)\n",
    "                ep_actor_losses.append(a_loss)\n",
    "                ep_q_values.append(q_val)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    if current_start_index < len(episode_cache):\n",
    "        final_achieved_goal = episode_cache[-1][\"achieved_goal\"]\n",
    "\n",
    "        for i in range(current_start_index, len(episode_cache)):\n",
    "            data = episode_cache[i]\n",
    "            is_last = i == len(episode_cache) - 1\n",
    "\n",
    "            # HER mechanism.\n",
    "            # we treat the last step as the desired goal\n",
    "            # adding \"fake\" labels into buffer\n",
    "            # hoping that agent will learn environment physics.\n",
    "            replay_buffer.add(\n",
    "                data[\"state\"],\n",
    "                data[\"action\"],\n",
    "                generic_task_ohe,\n",
    "                final_achieved_goal,\n",
    "                data[\"next_state\"],\n",
    "                reward=1 if is_last else 0,\n",
    "                done=1 if is_last else 0,\n",
    "            )\n",
    "\n",
    "            # adding \"real\" data, no relabeling here.\n",
    "            replay_buffer.add(\n",
    "                data[\"state\"],\n",
    "                data[\"action\"],\n",
    "                current_ohe,\n",
    "                data[\"desired_goal\"],\n",
    "                data[\"next_state\"],\n",
    "                reward=0,\n",
    "                done=0,\n",
    "            )\n",
    "\n",
    "    for task in TASKS:\n",
    "        if task in victory_map:\n",
    "            cumulative_victories[task] += 1\n",
    "        task_history[task].append(cumulative_victories[task])\n",
    "\n",
    "    # PLOT\n",
    "    if ep_critic_losses:\n",
    "        loss_history[\"critic_loss\"].append(np.mean(ep_critic_losses))\n",
    "        loss_history[\"actor_loss\"].append(np.mean(ep_actor_losses))\n",
    "        q_value_history.append(np.mean(ep_q_values))\n",
    "    else:\n",
    "        loss_history[\"critic_loss\"].append(0)\n",
    "        loss_history[\"actor_loss\"].append(0)\n",
    "        q_value_history.append(0)\n",
    "\n",
    "    if episode % HUMAN_DATA_FREQ == 0:\n",
    "        add_human_demonstrations(replay_buffer, OHE_LOOKUP, TASKS)\n",
    "\n",
    "    if episode % PLOT_FREQ == 0:\n",
    "        plt.clf()\n",
    "\n",
    "        for i, task in enumerate(TASKS):\n",
    "            plt.subplot(3, 4, i + 1)\n",
    "            plt.plot(task_history[task], label=\"cumulative reward\", color=\"blue\")\n",
    "            plt.title(f\"{task} cumulative reward\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 4, 8)\n",
    "        plt.plot(q_value_history, color=\"purple\")\n",
    "        plt.title(\"Avg Q-Value\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 4, 9)\n",
    "        plt.plot(loss_history[\"critic_loss\"][int(episode / 2) :], color=\"red\")\n",
    "        plt.title(\"Critic Loss (last 50%)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 4, 10)\n",
    "        plt.plot(loss_history[\"critic_loss\"], color=\"red\")\n",
    "        plt.title(\"Critic Loss\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 4, 11)\n",
    "        plt.plot(loss_history[\"actor_loss\"][int(episode / 2) :], color=\"green\")\n",
    "        plt.title(\"Actor Loss (last 50%)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 4, 12)\n",
    "        plt.plot(loss_history[\"actor_loss\"], color=\"green\")\n",
    "        plt.title(\"Actor Loss\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plot_display.update(plt.gcf())\n",
    "    \n",
    "    save_checkpoint(\n",
    "        CHECKPOINT_FREQ,\n",
    "        episode,\n",
    "        agent,\n",
    "        task_history,\n",
    "        cumulative_victories,\n",
    "        loss_history,\n",
    "        efficiency_history,\n",
    "        total_steps,\n",
    "        replay_buffer,\n",
    "        q_value_history,\n",
    "        CHECKPOINT_DIR,\n",
    "    )\n",
    "\n",
    "    c_loss_print = (\n",
    "        round(loss_history[\"critic_loss\"][-1], 5) if loss_history[\"critic_loss\"] else 0\n",
    "    )\n",
    "    print(\n",
    "        f\"Ep {episode}: Solved {list(victory_map.keys())} | Loss: {c_loss_print} | Buffer: {replay_buffer.size} | Unsolved: {current_task_name}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
