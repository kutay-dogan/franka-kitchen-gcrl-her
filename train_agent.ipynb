{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d35499",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import random\n",
    "import os\n",
    "from utils.functions import ohe_goals, flatten_info\n",
    "from utils.agent import DDPGAgent\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False \n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "ENV_NAME = 'FrankaKitchen-v1' \n",
    "MAX_EPISODES = 100000\n",
    "MAX_STEPS = 300\n",
    "BATCH_SIZE = 256\n",
    "START_TRAINING_AFTER = 2000\n",
    "NOISE = 0.05\n",
    "\n",
    "# Saving & Plotting Config\n",
    "CHECKPOINT_FREQ = 500\n",
    "PLOT_FREQ = 20\n",
    "RESUME_PATH = None \n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "TASKS = ['microwave', 'kettle', 'light switch', 'slide cabinet', 'hinge cabinet', 'top burner', 'bottom burner']\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "env = gym.make(ENV_NAME, max_episode_steps=MAX_STEPS, tasks_to_complete=TASKS, render_mode=\"human\")\n",
    "\n",
    "obs_dict = env.reset()[0]\n",
    "state_dim = obs_dict['observation'].shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "sample_goal = np.concatenate([obs_dict[\"desired_goal\"][k] for k in TASKS])\n",
    "goal_dim = sample_goal.shape[0]\n",
    "ohe_dim = len(TASKS)\n",
    "\n",
    "OHE_LOOKUP = ohe_goals(TASKS)\n",
    "device = \"cuda\"\n",
    "\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim, ohe_dim, goal_dim, device)\n",
    "agent = DDPGAgent(state_dim, action_dim, ohe_dim, goal_dim, device)\n",
    "\n",
    "\n",
    "task_history = {task: [0] for task in TASKS}\n",
    "cumulative_victories = {task: 0 for task in TASKS}\n",
    "\n",
    "loss_history = {\n",
    "    \"critic_loss\": [],\n",
    "    \"actor_loss\": []\n",
    "}\n",
    "\n",
    "efficiency_history = {task: [] for task in TASKS}\n",
    "\n",
    "start_episode = 0\n",
    "total_steps = 0\n",
    "\n",
    "plot_display = display.display(plt.figure(figsize=(16, 12)), display_id=True)\n",
    "\n",
    "if RESUME_PATH is not None and os.path.exists(RESUME_PATH):\n",
    "    print(f\"Loading checkpoint: {RESUME_PATH}\")\n",
    "    checkpoint = torch.load(RESUME_PATH)\n",
    "    \n",
    "    agent.actor.load_state_dict(checkpoint[\"actor_state_dict\"])\n",
    "    agent.critic.load_state_dict(checkpoint[\"critic_state_dict\"])\n",
    "    agent.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer_state_dict\"])\n",
    "    agent.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer_state_dict\"])\n",
    "    \n",
    "    task_history = checkpoint[\"task_history\"]\n",
    "    cumulative_victories = checkpoint[\"cumulative_reward\"]\n",
    "\n",
    "    loss_history = checkpoint.get(\"loss_history\", loss_history)\n",
    "    efficiency_history = checkpoint.get(\"efficiency_history\", efficiency_history)\n",
    "    \n",
    "    total_steps = checkpoint[\"total_steps\"]\n",
    "    start_episode = checkpoint[\"episode\"] + 1\n",
    "    print(f\"Resumed successfully from Episode {start_episode}\")\n",
    "\n",
    "print(f\"State Dim: {state_dim}, Goal Dim: {goal_dim}, OHE Dim: {ohe_dim}\")\n",
    "print(f\"Running On Device: {device}\")\n",
    "\n",
    "\n",
    "for episode in range(start_episode, MAX_EPISODES):\n",
    "    \n",
    "    raw_state, _ = env.reset()\n",
    "    state, achieved_goal, desired_goal = flatten_info(raw_state, TASKS)\n",
    "    \n",
    "    episode_cache = []\n",
    "    victory_map = {}\n",
    "    current_start_index = 0\n",
    "    undone_tasks = list(TASKS)\n",
    "\n",
    "    ep_critic_losses = []\n",
    "    ep_actor_losses = []\n",
    "    \n",
    "    current_task_name = np.random.choice(undone_tasks)\n",
    "    current_ohe = OHE_LOOKUP[current_task_name]\n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        \n",
    "        action = agent.select_action(state, current_ohe, desired_goal, noise=NOISE)\n",
    "        next_raw_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state, next_achieved, next_desired = flatten_info(next_raw_state, TASKS)\n",
    "        \n",
    "        episode_cache.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'next_state': next_state,\n",
    "            'achieved_goal': next_achieved, \n",
    "            'desired_goal': next_desired,   \n",
    "        })\n",
    "        \n",
    "        if info[\"step_task_completions\"]:\n",
    "            for task_name in info[\"step_task_completions\"]:\n",
    "                if task_name not in victory_map:\n",
    "                    victory_map[task_name] = step\n",
    "                    \n",
    "                    duration = step - current_start_index\n",
    "                    efficiency_history[task_name].append(duration)\n",
    "\n",
    "                    solved_ohe = OHE_LOOKUP[task_name]\n",
    "                    for i in range(current_start_index, step + 1):\n",
    "                        data = episode_cache[i]\n",
    "                        is_finish = (i == step)\n",
    "                        replay_buffer.add(\n",
    "                            data['state'], data['action'], solved_ohe,\n",
    "                            data['desired_goal'], data['next_state'],\n",
    "                            reward=1.0 if is_finish else 0.0,\n",
    "                            done=1.0 if is_finish else 0.0\n",
    "                        )\n",
    "                    \n",
    "                    current_start_index = step + 1\n",
    "                    if task_name in undone_tasks: \n",
    "                        undone_tasks.remove(task_name)\n",
    "                    \n",
    "                    if undone_tasks:\n",
    "                        current_task_name = np.random.choice(undone_tasks)\n",
    "                        current_ohe = OHE_LOOKUP[current_task_name]\n",
    "\n",
    "        state = next_state\n",
    "        total_steps += 1\n",
    "        \n",
    "        # Train Agent\n",
    "        if total_steps > START_TRAINING_AFTER:\n",
    "            c_loss, a_loss = agent.train(replay_buffer, BATCH_SIZE)\n",
    "            ep_critic_losses.append(c_loss)\n",
    "            ep_actor_losses.append(a_loss)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    # END OF EPISODE \n",
    "    if current_start_index < len(episode_cache):\n",
    "        final_achieved_goal = episode_cache[-1]['achieved_goal']\n",
    "        failed_ohe = current_ohe \n",
    "        \n",
    "        for i in range(current_start_index, len(episode_cache)):\n",
    "            data = episode_cache[i]\n",
    "            is_last = (i == len(episode_cache) - 1)\n",
    "            replay_buffer.add(\n",
    "                data['state'], data['action'], failed_ohe,\n",
    "                final_achieved_goal, data['next_state'],\n",
    "                reward=1.0 if is_last else 0.0,\n",
    "                done=1.0 if is_last else 0.0\n",
    "            )\n",
    "\n",
    "    for task in TASKS:\n",
    "        if task in victory_map:\n",
    "            cumulative_victories[task] += 1\n",
    "        task_history[task].append(cumulative_victories[task])\n",
    "    \n",
    "\n",
    "    if ep_critic_losses:\n",
    "        loss_history[\"critic_loss\"].append(np.mean(ep_critic_losses))\n",
    "        loss_history[\"actor_loss\"].append(np.mean(ep_actor_losses))\n",
    "    else:\n",
    "        loss_history[\"critic_loss\"].append(0)\n",
    "        loss_history[\"actor_loss\"].append(0)\n",
    "\n",
    "    if episode % PLOT_FREQ == 0:\n",
    "        plt.clf()\n",
    "        \n",
    "        for i, task in enumerate(TASKS):\n",
    "            plt.subplot(3, 4, i + 1) # 3 Rows, 4 Cols\n",
    "            plt.plot(task_history[task], label=\"Solves\", color=\"blue\")\n",
    "            plt.title(f\"{task} Cumulative Reward\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(3, 4, 9)\n",
    "        plt.plot(loss_history[\"critic_loss\"][int(MAX_EPISODES/2):], color=\"red\")\n",
    "        plt.title(\"Critic Loss (last 50%)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 4, 10)\n",
    "        plt.plot(loss_history[\"critic_loss\"], color=\"red\")\n",
    "        plt.title(\"Critic Loss\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(3, 4, 11)\n",
    "        plt.plot(loss_history[\"actor_loss\"][int(MAX_EPISODES/2):], color=\"green\")\n",
    "        plt.title(\"Actor Loss (last 50%)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 4, 12)\n",
    "        plt.plot(loss_history[\"actor_loss\"], color=\"green\")\n",
    "        plt.title(\"Actor Loss\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_display.update(plt.gcf())\n",
    "\n",
    "    if (episode + 1) % CHECKPOINT_FREQ == 0:\n",
    "        checkpoint_data = {\n",
    "            \"episode\": episode,\n",
    "            \"actor_state_dict\": agent.actor.state_dict(),\n",
    "            \"critic_state_dict\": agent.critic.state_dict(),\n",
    "            \"actor_optimizer_state_dict\": agent.actor_optimizer.state_dict(),\n",
    "            \"critic_optimizer_state_dict\": agent.critic_optimizer.state_dict(),\n",
    "            \"task_history\": task_history,\n",
    "            \"cumulative_reward\": cumulative_victories, \n",
    "            \"loss_history\": loss_history,\n",
    "            \"efficiency_history\": efficiency_history,\n",
    "            \"total_steps\": total_steps,\n",
    "        }\n",
    "        save_filename = f\"{CHECKPOINT_DIR}/checkpoint_{episode + 1}.pth\"\n",
    "        torch.save(checkpoint_data, save_filename)\n",
    "        print(f\"--> Checkpoint saved: {save_filename}\")\n",
    "\n",
    "    c_loss_print = round(loss_history[\"critic_loss\"][-1], 5) if loss_history[\"critic_loss\"] else 0\n",
    "    print(f\"Ep {episode}: Solved {list(victory_map.keys())} | Loss: {c_loss_print} | Buffer: {replay_buffer.size} | Unsolved: {current_task_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
