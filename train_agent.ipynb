{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d35499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint: checkpoints/checkpoint_1500.pth\n",
      "Resumed successfully from Episode 1500\n",
      "State Dim: 59, Goal Dim: 17, OHE Dim: 7\n",
      "Running On Device: mps\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "high <= 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 169\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_steps \u001b[38;5;241m>\u001b[39m START_TRAINING_AFTER:\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(K):\n\u001b[0;32m--> 169\u001b[0m         c_loss, a_loss \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplay_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m         ep_critic_losses\u001b[38;5;241m.\u001b[39mappend(c_loss)\n\u001b[1;32m    171\u001b[0m         ep_actor_losses\u001b[38;5;241m.\u001b[39mappend(a_loss)\n",
      "File \u001b[0;32m~/Documents/GitHub/franka-foresight/utils/agent.py:49\u001b[0m, in \u001b[0;36mDDPGAgent.train\u001b[0;34m(self, replay_buffer, batch_size)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, replay_buffer, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     state, action, ohe, goal, next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43mreplay_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m     52\u001b[0m         next_action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactor_target(next_state, ohe, goal)\n",
      "File \u001b[0;32m~/Documents/GitHub/franka-foresight/utils/replay_buffer.py:35\u001b[0m, in \u001b[0;36mReplayBuffer.sample\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_size):\n\u001b[1;32m     32\u001b[0m     \n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m#ind = np.random.choice(self.size, size=batch_size, replace=False)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# sampling without with replacement is probably a faster approach.\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m     ind \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m     38\u001b[0m         torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate[ind])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m     39\u001b[0m         torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction[ind])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     44\u001b[0m         torch\u001b[38;5;241m.\u001b[39mFloatTensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone[ind])\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice),\n\u001b[1;32m     45\u001b[0m     )\n",
      "File \u001b[0;32mnumpy/random/mtrand.pyx:796\u001b[0m, in \u001b[0;36mnumpy.random.mtrand.RandomState.randint\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mnumpy/random/_bounded_integers.pyx:1334\u001b[0m, in \u001b[0;36mnumpy.random._bounded_integers._rand_int64\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: high <= 0"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1600x1200 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import gymnasium_robotics\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import random\n",
    "import os\n",
    "from utils.functions import ohe_goals, flatten_info\n",
    "from utils.agent import DDPGAgent\n",
    "from utils.replay_buffer import ReplayBuffer\n",
    "\n",
    "SEED = 42\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False \n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "ENV_NAME = 'FrankaKitchen-v1' \n",
    "MAX_EPISODES = 20000\n",
    "MAX_STEPS = 300\n",
    "BATCH_SIZE = 256\n",
    "START_TRAINING_AFTER = 2000\n",
    "NOISE = 0.05\n",
    "K = 1\n",
    "\n",
    "# Saving & Plotting Config\n",
    "CHECKPOINT_FREQ = 500\n",
    "PLOT_FREQ = 10\n",
    "RESUME_PATH = \"checkpoints/checkpoint_1500.pth\"\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "TASKS = ['microwave', 'kettle', 'light switch', 'slide cabinet', 'hinge cabinet', 'top burner', 'bottom burner']\n",
    "\n",
    "if not os.path.exists(CHECKPOINT_DIR):\n",
    "    os.makedirs(CHECKPOINT_DIR)\n",
    "\n",
    "env = gym.make(ENV_NAME, max_episode_steps=MAX_STEPS, tasks_to_complete=TASKS)\n",
    "\n",
    "obs_dict = env.reset()[0]\n",
    "state_dim = obs_dict['observation'].shape[0]\n",
    "action_dim = env.action_space.shape[0]\n",
    "sample_goal = np.concatenate([obs_dict[\"desired_goal\"][k] for k in TASKS])\n",
    "goal_dim = sample_goal.shape[0]\n",
    "ohe_dim = len(TASKS)\n",
    "\n",
    "OHE_LOOKUP = ohe_goals(TASKS)\n",
    "device = \"mps\"\n",
    "\n",
    "replay_buffer = ReplayBuffer(state_dim, action_dim, ohe_dim, goal_dim, device)\n",
    "agent = DDPGAgent(state_dim, action_dim, ohe_dim, goal_dim, device)\n",
    "\n",
    "\n",
    "task_history = {task: [0] for task in TASKS}\n",
    "cumulative_victories = {task: 0 for task in TASKS}\n",
    "\n",
    "loss_history = {\n",
    "    \"critic_loss\": [],\n",
    "    \"actor_loss\": []\n",
    "}\n",
    "\n",
    "efficiency_history = {task: [] for task in TASKS}\n",
    "\n",
    "start_episode = 0\n",
    "total_steps = 0\n",
    "\n",
    "plot_display = display.display(plt.figure(figsize=(16, 12)), display_id=True)\n",
    "\n",
    "if RESUME_PATH is not None and os.path.exists(RESUME_PATH):\n",
    "    print(f\"Loading checkpoint: {RESUME_PATH}\")\n",
    "    checkpoint = torch.load(RESUME_PATH, weights_only=False)\n",
    "    \n",
    "    agent.actor.load_state_dict(checkpoint[\"actor_state_dict\"])\n",
    "    agent.critic.load_state_dict(checkpoint[\"critic_state_dict\"])\n",
    "    agent.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer_state_dict\"])\n",
    "    agent.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer_state_dict\"])\n",
    "    \n",
    "    task_history = checkpoint[\"task_history\"]\n",
    "    cumulative_victories = checkpoint[\"cumulative_reward\"]\n",
    "\n",
    "    loss_history = checkpoint.get(\"loss_history\", loss_history)\n",
    "    efficiency_history = checkpoint.get(\"efficiency_history\", efficiency_history)\n",
    "    \n",
    "    total_steps = checkpoint[\"total_steps\"]\n",
    "    start_episode = checkpoint[\"episode\"] + 1\n",
    "    replay_buffer = checkpoint[\"replay_buffer\"]\n",
    "    print(f\"Resumed successfully from Episode {start_episode}\")\n",
    "\n",
    "print(f\"State Dim: {state_dim}, Goal Dim: {goal_dim}, OHE Dim: {ohe_dim}\")\n",
    "print(f\"Running On Device: {device}\")\n",
    "\n",
    "\n",
    "for episode in range(start_episode, MAX_EPISODES):\n",
    "    \n",
    "    raw_state, _ = env.reset()\n",
    "    state, achieved_goal, desired_goal = flatten_info(raw_state, TASKS)\n",
    "    \n",
    "    episode_cache = []\n",
    "    victory_map = {}\n",
    "    current_start_index = 0\n",
    "    undone_tasks = list(TASKS)\n",
    "\n",
    "    ep_critic_losses = []\n",
    "    ep_actor_losses = []\n",
    "    \n",
    "    current_task_name = np.random.choice(undone_tasks)\n",
    "    current_ohe = OHE_LOOKUP[current_task_name]\n",
    "    generic_task_ohe = np.zeros(ohe_dim) \n",
    "    \n",
    "    for step in range(MAX_STEPS):\n",
    "        \n",
    "        action = agent.select_action(state, current_ohe, desired_goal, noise=NOISE)\n",
    "        next_raw_state, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state, next_achieved, next_desired = flatten_info(next_raw_state, TASKS)\n",
    "        \n",
    "        episode_cache.append({\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'next_state': next_state,\n",
    "            'achieved_goal': next_achieved, \n",
    "            'desired_goal': next_desired,   \n",
    "        })\n",
    "        \n",
    "        if info[\"step_task_completions\"]:\n",
    "            for task_name in info[\"step_task_completions\"]:\n",
    "                if task_name not in victory_map:\n",
    "                    victory_map[task_name] = step\n",
    "                    \n",
    "                    duration = step - current_start_index\n",
    "                    efficiency_history[task_name].append(duration)\n",
    "\n",
    "                    solved_ohe = OHE_LOOKUP[task_name]\n",
    "                    for i in range(current_start_index, step + 1):\n",
    "                        data = episode_cache[i]\n",
    "                        is_finish = (i == step)\n",
    "                        replay_buffer.add(\n",
    "                            data['state'], \n",
    "                            data['action'], \n",
    "                            solved_ohe,\n",
    "                            data['desired_goal'], \n",
    "                            data['next_state'],\n",
    "                            reward=1.0 if is_finish else 0.0,\n",
    "                            done=1.0 if is_finish else 0.0\n",
    "                        )\n",
    "                    \n",
    "                    current_start_index = step + 1\n",
    "                    if task_name in undone_tasks: \n",
    "                        undone_tasks.remove(task_name)\n",
    "                    \n",
    "                    if undone_tasks:\n",
    "                        current_task_name = np.random.choice(undone_tasks)\n",
    "                        current_ohe = OHE_LOOKUP[current_task_name]\n",
    "\n",
    "        state = next_state\n",
    "        total_steps += 1\n",
    "        \n",
    "        # Train Agent\n",
    "        if total_steps > START_TRAINING_AFTER:\n",
    "            for _ in range(K):\n",
    "                c_loss, a_loss = agent.train(replay_buffer, BATCH_SIZE)\n",
    "                ep_critic_losses.append(c_loss)\n",
    "                ep_actor_losses.append(a_loss)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "\n",
    "    if current_start_index < len(episode_cache):\n",
    "        final_achieved_goal = episode_cache[-1]['achieved_goal']\n",
    "\n",
    "        for i in range(current_start_index, len(episode_cache)):\n",
    "            data = episode_cache[i]\n",
    "            is_last = (i == len(episode_cache) - 1)\n",
    "            \n",
    "            replay_buffer.add(\n",
    "                data['state'], \n",
    "                data['action'], \n",
    "                generic_task_ohe,     \n",
    "                final_achieved_goal,  \n",
    "                data['next_state'],\n",
    "                reward=1.0 if is_last else 0.0,\n",
    "                done=1.0 if is_last else 0.0\n",
    "            )\n",
    "        \n",
    "            replay_buffer.add(\n",
    "                data['state'], \n",
    "                data['action'], \n",
    "                current_ohe, \n",
    "                data['desired_goal'],\n",
    "                data['next_state'],\n",
    "                reward=0.0,\n",
    "                done=0.0\n",
    "            )\n",
    "\n",
    "    for task in TASKS:\n",
    "        if task in victory_map:\n",
    "            cumulative_victories[task] += 1\n",
    "        task_history[task].append(cumulative_victories[task])\n",
    "    \n",
    "\n",
    "    if ep_critic_losses:\n",
    "        loss_history[\"critic_loss\"].append(np.mean(ep_critic_losses))\n",
    "        loss_history[\"actor_loss\"].append(np.mean(ep_actor_losses))\n",
    "    else:\n",
    "        loss_history[\"critic_loss\"].append(0)\n",
    "        loss_history[\"actor_loss\"].append(0)\n",
    "\n",
    "    if episode % PLOT_FREQ == 0:\n",
    "        plt.clf()\n",
    "        \n",
    "        for i, task in enumerate(TASKS):\n",
    "            plt.subplot(3, 4, i + 1) # 3 Rows, 4 Cols\n",
    "            plt.plot(task_history[task], label=\"cumulative reward\", color=\"blue\")\n",
    "            plt.title(f\"{task} cumulative reward\")\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(3, 4, 9)\n",
    "        plt.plot(loss_history[\"critic_loss\"][int(MAX_EPISODES/2):], color=\"red\")\n",
    "        plt.title(\"Critic Loss (last 50%)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 4, 10)\n",
    "        plt.plot(loss_history[\"critic_loss\"], color=\"red\")\n",
    "        plt.title(\"Critic Loss\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(3, 4, 11)\n",
    "        plt.plot(loss_history[\"actor_loss\"][int(MAX_EPISODES/2):], color=\"green\")\n",
    "        plt.title(\"Actor Loss (last 50%)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.subplot(3, 4, 12)\n",
    "        plt.plot(loss_history[\"actor_loss\"], color=\"green\")\n",
    "        plt.title(\"Actor Loss\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_display.update(plt.gcf())\n",
    "\n",
    "    if (episode + 1) % CHECKPOINT_FREQ == 0:\n",
    "        checkpoint_data = {\n",
    "            \"episode\": episode,\n",
    "            \"actor_state_dict\": agent.actor.state_dict(),\n",
    "            \"critic_state_dict\": agent.critic.state_dict(),\n",
    "            \"actor_optimizer_state_dict\": agent.actor_optimizer.state_dict(),\n",
    "            \"critic_optimizer_state_dict\": agent.critic_optimizer.state_dict(),\n",
    "            \"task_history\": task_history,\n",
    "            \"cumulative_reward\": cumulative_victories, \n",
    "            \"loss_history\": loss_history,\n",
    "            \"efficiency_history\": efficiency_history,\n",
    "            \"total_steps\": total_steps,\n",
    "            \"replay_buffer\": replay_buffer\n",
    "        }\n",
    "        save_filename = f\"{CHECKPOINT_DIR}/checkpoint_{episode + 1}.pth\"\n",
    "        torch.save(checkpoint_data, save_filename)\n",
    "        print(f\"--> Checkpoint saved: {save_filename}\")\n",
    "\n",
    "    c_loss_print = round(loss_history[\"critic_loss\"][-1], 5) if loss_history[\"critic_loss\"] else 0\n",
    "    print(f\"Ep {episode}: Solved {list(victory_map.keys())} | Loss: {c_loss_print} | Buffer: {replay_buffer.size} | Unsolved: {current_task_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80f1c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
